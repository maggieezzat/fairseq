Issue: https://github.com/pytorch/fairseq/issues/3005

I also experienced some problems during inference, caused by the script in fairseq/tasks/audio_pretraining

        manifest = os.path.join(data_path, "{}.tsv".format(split))
        self.datasets[split] = FileAudioDataset(
            manifest,
            sample_rate=task_cfg.sample_rate,
            max_sample_size=self.cfg.max_sample_size,
            min_sample_size=self.cfg.max_sample_size,
            min_length=self.cfg.min_sample_size,
            pad=task_cfg.labels is not None or task_cfg.enable_padding,
            normalize=task_cfg.normalize,
        )
sample_rate and normalize are not attributes of task_cfg, so I fix it by:

sample_rate=task_cfg.sample_rate ---> sample_rate=self.cfg.sample_rate
normalize=False
and also:

# upgrade old task
if isinstance(task_cfg, Namespace):
    if not hasattr(task_cfg, "autoregressive"):
        task_cfg.autoregressive = not task_cfg.criterion == 'ctc'
task_cfg was not isinstance of Namespace. Therefore, task_cfg.autoregressive is not set after the above code, which caused some errors later. I fix it by removing the "if " condition:

task_cfg.autoregressive = not task_cfg.criterion == 'ctc'


COST OF PRETRAINING: 
B ASE contains 12 transformer blocks, model dimension 768, inner dimension(FFN) 3,072 and 8 attention heads
we train on a total of 64 V100 GPUs for 1.6 days (BASE on 960 hrs)

